#!/usr/bin/env python3
"""
RAGçŸ¥è¯†åº“ç³»ç»Ÿ - Flask Web APIæœåŠ¡
é›†æˆOllamaå®¢æˆ·ç«¯ã€æ–‡æ¡£å¤„ç†å™¨å’ŒQdrantå‘é‡å­˜å‚¨
æä¾›æ–‡ä»¶ä¸Šä¼ ã€å¯¹è¯ã€æ–‡æ¡£ç®¡ç†ç­‰RESTful APIæ¥å£
"""

import os
import json
import uuid
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from werkzeug.utils import secure_filename
from werkzeug.exceptions import RequestEntityTooLarge

# å¯¼å…¥ç°æœ‰çš„RAGç³»ç»Ÿç»„ä»¶
from ollama_client import OllamaClient
from document_processor import DocumentProcessor
from vector_store import VectorStore
from database import DatabaseManager


class RAGWebService:
    """
    RAG WebæœåŠ¡ç±»
    ç®¡ç†RAGç³»ç»Ÿçš„webæ¥å£å’Œä¸šåŠ¡é€»è¾‘
    """
    
    def __init__(self):
        """åˆå§‹åŒ–RAG WebæœåŠ¡"""
        self.ollama_client = None
        self.document_processor = None
        self.vector_store = None
        self.db_manager = None  # æ•°æ®åº“ç®¡ç†å™¨
        self.documents = {}  # ä¸´æ—¶å­˜å‚¨æ–‡æ¡£ä¿¡æ¯ï¼ˆå‘åå…¼å®¹ï¼‰
        self.chat_history = []  # ä¸´æ—¶å­˜å‚¨å¯¹è¯å†å²ï¼ˆå‘åå…¼å®¹ï¼‰
        
        # åˆ›å»ºä¸Šä¼ ç›®å½•
        self.upload_folder = Path("uploads")
        self.upload_folder.mkdir(exist_ok=True)
        
        # å…è®¸çš„æ–‡ä»¶ç±»å‹
        self.allowed_extensions = {'txt', 'pdf', 'docx', 'md'}
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
    
    def _initialize_components(self) -> bool:
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿç»„ä»¶
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            print("æ­£åœ¨åˆå§‹åŒ–RAGç³»ç»Ÿç»„ä»¶...")
            
            # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
            try:
                self.db_manager = DatabaseManager()
                print("âœ… æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                # æ•°æ®åº“å¤±è´¥ä¸é˜»æ­¢å¯åŠ¨ï¼Œç»§ç»­åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
            
            # åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯
            try:
                self.ollama_client = OllamaClient()
                if not self.ollama_client.check_connection():
                    print("âš ï¸ Ollamaè¿æ¥å¤±è´¥ï¼Œä½†ç»§ç»­å¯åŠ¨æœåŠ¡")
                else:
                    print("âœ… Ollamaå®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
            except Exception as e:
                print(f"âŒ Ollamaå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿå®¢æˆ·ç«¯ä»¥é¿å…Noneé”™è¯¯
                class MockOllamaClient:
                    def __init__(self):
                        self.host = 'localhost'
                        self.port = '11434'
                        self.chat_model = 'qwen3:14b'
                        self.embedding_model = 'dengcao/Qwen3-Embedding-4B:Q8_0'
                        self.is_mock = True
                    
                    def get_embedding(self, text):
                        # è¿”å›Noneï¼Œè®©ä¸Šå±‚é€»è¾‘å¤„ç†
                        return None
                    
                    def generate_response(self, prompt, context=""):
                        return "æŠ±æ­‰ï¼ŒOllamaæœåŠ¡æœªè¿æ¥ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚è¯·æ£€æŸ¥Ollamaæ˜¯å¦æ­£å¸¸è¿è¡Œã€‚"
                    
                    def generate_stream_response(self, prompt, context=""):
                        # è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œäº§ç”Ÿé”™è¯¯æ¶ˆæ¯
                        yield "æŠ±æ­‰ï¼ŒOllamaæœåŠ¡æœªè¿æ¥ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚è¯·æ£€æŸ¥Ollamaæ˜¯å¦æ­£å¸¸è¿è¡Œã€‚"
                
                self.ollama_client = MockOllamaClient()
            
            # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
            try:
                self.document_processor = DocumentProcessor()
                print("âœ… æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # åˆå§‹åŒ–å‘é‡å­˜å‚¨
            try:
                self.vector_store = VectorStore()
                print("âœ… å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def is_allowed_file(self, filename: str) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦å…è®¸
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            bool: æ˜¯å¦å…è®¸çš„æ–‡ä»¶ç±»å‹
        """
        return '.' in filename and \
               filename.rsplit('.', 1)[1].lower() in self.allowed_extensions
    
    def safe_filename(self, filename: str) -> str:
        """
        å®‰å…¨å¤„ç†æ–‡ä»¶åï¼Œä¿ç•™æ‰©å±•å
        
        Args:
            filename: åŸå§‹æ–‡ä»¶å
            
        Returns:
            str: å®‰å…¨çš„æ–‡ä»¶å
        """
        if '.' in filename:
            name, ext = filename.rsplit('.', 1)
            # ä½¿ç”¨secure_filenameå¤„ç†æ–‡ä»¶åéƒ¨åˆ†
            safe_name = secure_filename(name)
            # ç¡®ä¿æ‰©å±•åå®‰å…¨ä¸”ä¿æŒåŸæ ·
            safe_ext = ''.join(c for c in ext if c.isalnum()).lower()
            return f"{safe_name}.{safe_ext}" if safe_name else f"file.{safe_ext}"
        else:
            return secure_filename(filename) or "file"
    
    def upload_document(self, file) -> Dict[str, Any]:
        """
        ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£
        
        Args:
            file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            
        Returns:
            Dict: å¤„ç†ç»“æœ
        """
        try:
            if not file or file.filename == '':
                return {"success": False, "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"}
            
            if not self.is_allowed_file(file.filename):
                return {"success": False, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹"}
            
            # ä¿å­˜æ–‡ä»¶
            original_filename = file.filename  # ä¿å­˜åŸå§‹æ–‡ä»¶åç”¨äºæ˜¾ç¤º
            safe_filename = self.safe_filename(file.filename)  # å®‰å…¨æ–‡ä»¶åç”¨äºå¤„ç†
            file_id = str(uuid.uuid4())
            # æ–‡ä»¶å­˜å‚¨æ—¶ä½¿ç”¨UUIDä½œä¸ºæ–‡ä»¶åï¼Œä¿ç•™æ‰©å±•å
            file_extension = ''
            if '.' in original_filename:
                file_extension = '.' + original_filename.rsplit('.', 1)[1].lower()
            file_path = self.upload_folder / f"{file_id}{file_extension}"
            file.save(str(file_path))
            
            # å¤„ç†æ–‡æ¡£
            print(f"æ­£åœ¨å¤„ç†æ–‡æ¡£: {original_filename}")
            chunks = self.document_processor.process_document(str(file_path))
            
            if not chunks:
                return {"success": False, "error": "æ–‡æ¡£å¤„ç†å¤±è´¥æˆ–æ–‡æ¡£ä¸ºç©º"}
            
            # ç”Ÿæˆå‘é‡å¹¶å­˜å‚¨
            print(f"æ­£åœ¨ç”Ÿæˆå‘é‡ï¼Œå…±{len(chunks)}ä¸ªæ–‡æ¡£å—...")
            vectors = []
            for i, chunk in enumerate(chunks):
                try:
                    vector = self.ollama_client.get_embedding(chunk['text'])
                    if vector:
                        vectors.append({
                            "id": f"{file_id}_{i}",
                            "vector": vector,
                            "payload": {
                                "text": chunk['text'],
                                "file_id": file_id,
                                "filename": original_filename,
                                "chunk_index": i,
                                "upload_time": datetime.now().isoformat()
                            }
                        })
                except Exception as e:
                    print(f"å¤„ç†ç¬¬{i}ä¸ªæ–‡æ¡£å—æ—¶å‡ºé”™: {e}")
                    continue
            
            if not vectors:
                return {"success": False, "error": "å‘é‡ç”Ÿæˆå¤±è´¥"}
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            # åˆ†ç¦»å‘é‡å’Œå…ƒæ•°æ®
            vector_list = [item["vector"] for item in vectors]
            metadata_list = [item["payload"] for item in vectors]
            success = self.vector_store.add_vectors(vector_list, metadata_list)
            if not success:
                return {"success": False, "error": "å‘é‡å­˜å‚¨å¤±è´¥"}
            
            # è®°å½•æ–‡æ¡£ä¿¡æ¯
            doc_info = {
                "id": file_id,
                "filename": original_filename,  # ä¿å­˜åŸå§‹æ–‡ä»¶åç”¨äºæ˜¾ç¤º
                "original_filename": original_filename,
                "file_path": str(file_path),
                "chunks_count": len(chunks),
                "vectors_count": len(vectors),
                "upload_time": datetime.now().isoformat(),
                "file_size": file_path.stat().st_size
            }
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            self.db_manager.add_document(doc_info)
            
            # ä¸´æ—¶ä¿å­˜åˆ°å†…å­˜ï¼ˆå‘åå…¼å®¹ï¼‰
            self.documents[file_id] = doc_info
            
            print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {original_filename}, ç”Ÿæˆ{len(vectors)}ä¸ªå‘é‡")
            
            return {
                "success": True,
                "file_id": file_id,
                "filename": original_filename,  # è¿”å›åŸå§‹æ–‡ä»¶å
                "chunks_count": len(chunks),
                "vectors_count": len(vectors)
            }
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
            traceback.print_exc()
            return {"success": False, "error": f"å¤„ç†å¤±è´¥: {str(e)}"}
    
    def clear_knowledge_base(self) -> Dict[str, Any]:
        """
        æ¸…ç†çŸ¥è¯†åº“å’Œé‡ç½®Qdrant
        
        Returns:
            Dict: æ¸…ç†ç»“æœ
        """
        try:
            print("æ­£åœ¨æ¸…ç†çŸ¥è¯†åº“...")
            
            # æ¸…ç†å‘é‡å­˜å‚¨
            if self.vector_store:
                success = self.vector_store.clear_collection()
                if not success:
                    return {"success": False, "error": "å‘é‡å­˜å‚¨æ¸…ç†å¤±è´¥"}
            
            # æ¸…ç†æ•°æ®åº“ä¸­çš„æ–‡æ¡£è®°å½•
            if self.db_manager:
                self.db_manager.clear_all_documents()
            
            # æ¸…ç†å†…å­˜ä¸­çš„æ–‡æ¡£è®°å½•ï¼ˆå‘åå…¼å®¹ï¼‰
            self.documents.clear()
            
            # æ¸…ç†å¯¹è¯å†å²
            self.chat_history.clear()
            
            # æ¸…ç†ä¸Šä¼ çš„æ–‡ä»¶
            try:
                for file_path in self.upload_folder.glob("*"):
                    if file_path.is_file():
                        file_path.unlink()
                print("âœ… ä¸Šä¼ æ–‡ä»¶æ¸…ç†å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†ä¸Šä¼ æ–‡ä»¶æ—¶å‡ºç°è­¦å‘Š: {e}")
            
            print("âœ… çŸ¥è¯†åº“æ¸…ç†å®Œæˆ")
            
            return {
                "success": True,
                "message": "çŸ¥è¯†åº“å·²æˆåŠŸæ¸…ç†",
                "cleared_items": {
                    "documents": True,
                    "vectors": True,
                    "chat_history": True,
                    "uploaded_files": True
                }
            }
            
        except Exception as e:
            print(f"âŒ çŸ¥è¯†åº“æ¸…ç†å¤±è´¥: {e}")
            traceback.print_exc()
            return {"success": False, "error": f"æ¸…ç†å¤±è´¥: {str(e)}"}

    def parse_model_response(self, raw_response: str) -> tuple[str, str]:
        """
        è§£ææ¨¡å‹å›ç­”ï¼Œåˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
        
        Args:
            raw_response: æ¨¡å‹çš„åŸå§‹å›ç­”
            
        Returns:
            tuple: (thinking, final_answer)
        """
        if not raw_response or not raw_response.strip():
            return "", ""
        
        # æŒ‰è¡Œåˆ†å‰²å›ç­”
        lines = raw_response.strip().split('\n')
        
        # è¿‡æ»¤ç©ºè¡Œ
        non_empty_lines = [line.strip() for line in lines if line.strip()]
        
        if not non_empty_lines:
            return "", ""
        
        # å¦‚æœåªæœ‰ä¸€è¡Œï¼Œç›´æ¥ä½œä¸ºæœ€ç»ˆç­”æ¡ˆ
        if len(non_empty_lines) == 1:
            return "", non_empty_lines[0]
        
        # æœ€åä¸€è¡Œä½œä¸ºæœ€ç»ˆç­”æ¡ˆ
        final_answer = non_empty_lines[-1]
        
        # å‰é¢çš„æ‰€æœ‰è¡Œä½œä¸ºæ€è€ƒè¿‡ç¨‹
        thinking_lines = non_empty_lines[:-1]
        thinking = '\n'.join(thinking_lines)
        
        return thinking, final_answer

    def chat(self, question: str, top_k: int = 1) -> Dict[str, Any]:
        """
        å¤„ç†å¯¹è¯è¯·æ±‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: è¿”å›çš„ç›¸å…³æ–‡æ¡£æ•°é‡
            
        Returns:
            Dict: å¯¹è¯ç»“æœ
        """
        try:
            if not question.strip():
                return {"success": False, "error": "é—®é¢˜ä¸èƒ½ä¸ºç©º"}
            
            # ç”Ÿæˆé—®é¢˜çš„å‘é‡
            question_vector = self.ollama_client.get_embedding(question)
            if not question_vector:
                return {"success": False, "error": "é—®é¢˜å‘é‡åŒ–å¤±è´¥"}
            
            # æœç´¢ç›¸å…³æ–‡æ¡£
            search_results = self.vector_store.search_similar(
                query_vector=question_vector,
                top_k=top_k,
                score_threshold=0.3
            )
            
            if not search_results:
                # æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹ç›´æ¥å›ç­”
                system_thinking = f"æ²¡æœ‰åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ä¸é—®é¢˜'{question}'ç›¸å…³çš„æ–‡æ¡£ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œå›ç­”ã€‚"
                prompt = f"è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}"
                raw_response = self.ollama_client.generate_response(prompt)
                if not raw_response:
                    return {"success": False, "error": "å›ç­”ç”Ÿæˆå¤±è´¥"}
                
                # è§£ææ¨¡å‹å›ç­”
                model_thinking, final_answer = self.parse_model_response(raw_response)
                
                # åˆå¹¶ç³»ç»Ÿæ€è€ƒå’Œæ¨¡å‹æ€è€ƒ
                combined_thinking = system_thinking
                if model_thinking:
                    combined_thinking += "\n\næ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼š\n" + model_thinking
                
                return {
                    "success": True,
                    "answer": final_answer,
                    "sources": [],
                    "question": question,
                    "mode": "åŸºç¡€æ¨¡å‹å›ç­”",
                    "thinking": combined_thinking
                }
            
            # æ„å»ºä¸Šä¸‹æ–‡ï¼Œè¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
            context_parts = []
            sources = []
            
            for result in search_results:
                payload = result.get('payload', {})
                text = payload.get('text', '')
                filename = payload.get('filename', 'æœªçŸ¥æ–‡ä»¶')
                chunk_index = payload.get('chunk_index', 0)
                score = result.get('score', 0)
                
                # è¿‡æ»¤ç›¸ä¼¼åº¦ä½äº0.5çš„ç»“æœ
                if score < 0.5:
                    continue
                    
                context_parts.append(text)
                sources.append({
                    "filename": filename,
                    "chunk_index": chunk_index,
                    "score": round(score, 3),
                    "text_preview": text[:200] + "..." if len(text) > 200 else text
                })
            
            # æ£€æŸ¥è¿‡æ»¤åæ˜¯å¦è¿˜æœ‰æœ‰æ•ˆç»“æœ
            if not context_parts:
                # æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„æ–‡æ¡£ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹ç›´æ¥å›ç­”
                system_thinking = f"è™½ç„¶æ‰¾åˆ°äº†{len(search_results)}ä¸ªç›¸å…³æ–‡æ¡£ï¼Œä½†ç›¸ä¼¼åº¦éƒ½ä½äº0.5é˜ˆå€¼ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œå›ç­”ã€‚"
                prompt = f"è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}"
                raw_response = self.ollama_client.generate_response(prompt)
                if not raw_response:
                    return {"success": False, "error": "å›ç­”ç”Ÿæˆå¤±è´¥"}
                
                # è§£ææ¨¡å‹å›ç­”
                model_thinking, final_answer = self.parse_model_response(raw_response)
                
                # åˆå¹¶ç³»ç»Ÿæ€è€ƒå’Œæ¨¡å‹æ€è€ƒ
                combined_thinking = system_thinking
                if model_thinking:
                    combined_thinking += "\n\næ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼š\n" + model_thinking
                
                return {
                    "success": True,
                    "answer": final_answer,
                    "sources": [],
                    "question": question,
                    "mode": "åŸºç¡€æ¨¡å‹å›ç­”",
                    "thinking": combined_thinking
                }
            
            context = "\n\n".join(context_parts)
            
            # ç”Ÿæˆç³»ç»Ÿæ€è€ƒå†…å®¹
            system_thinking = f"åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°äº†{len(sources)}ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œç›¸ä¼¼åº¦èŒƒå›´ï¼š{min([s['score'] for s in sources]):.3f}-{max([s['score'] for s in sources]):.3f}ã€‚åŸºäºè¿™äº›æ–‡æ¡£å†…å®¹æ¥å›ç­”é—®é¢˜ã€‚"
            
            # ç”Ÿæˆå›ç­”
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ï¼š"""
            
            raw_response = self.ollama_client.generate_response(prompt)
            if not raw_response:
                return {"success": False, "error": "å›ç­”ç”Ÿæˆå¤±è´¥"}
            
            # è§£ææ¨¡å‹å›ç­”
            model_thinking, final_answer = self.parse_model_response(raw_response)
            
            # åˆå¹¶ç³»ç»Ÿæ€è€ƒå’Œæ¨¡å‹æ€è€ƒ
            combined_thinking = system_thinking
            if model_thinking:
                combined_thinking += "\n\næ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼š\n" + model_thinking
            
            # è®°å½•å¯¹è¯å†å²
            chat_record = {
                "id": str(uuid.uuid4()),
                "question": question,
                "answer": final_answer,
                "sources": sources,
                "timestamp": datetime.now().isoformat()
            }
            self.chat_history.append(chat_record)
            
            return {
                "success": True,
                "answer": final_answer,
                "sources": sources,
                "question": question,
                "mode": "çŸ¥è¯†åº“å›ç­”",
                "thinking": combined_thinking
            }
            
        except Exception as e:
            print(f"âŒ å¯¹è¯å¤„ç†å¤±è´¥: {e}")
            traceback.print_exc()
            return {"success": False, "error": f"å¯¹è¯å¤„ç†å¤±è´¥: {str(e)}"}
    
    def get_documents(self, page: int = 1, page_size: int = 10) -> Dict[str, Any]:
        """
        è·å–å·²ä¸Šä¼ çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆåˆ†é¡µï¼‰
        
        Args:
            page: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
            page_size: æ¯é¡µæ•°é‡
            
        Returns:
            Dict: åŒ…å«æ–‡æ¡£åˆ—è¡¨å’Œåˆ†é¡µä¿¡æ¯
        """
        try:
            documents, total = self.db_manager.get_documents(page, page_size)
            total_pages = (total + page_size - 1) // page_size  # å‘ä¸Šå–æ•´
            
            return {
                "success": True,
                "documents": documents,
                "pagination": {
                    "current_page": page,
                    "page_size": page_size,
                    "total_items": total,
                    "total_pages": total_pages,
                    "has_next": page < total_pages,
                    "has_prev": page > 1
                }
            }
        except Exception as e:
            print(f"âŒ è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "documents": [],
                "pagination": {
                    "current_page": 1,
                    "page_size": page_size,
                    "total_items": 0,
                    "total_pages": 0,
                    "has_next": False,
                    "has_prev": False
                }
            }
    
    def delete_document(self, file_id: str) -> Dict[str, Any]:
        """
        åˆ é™¤æ–‡æ¡£
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            Dict: åˆ é™¤ç»“æœ
        """
        try:
            # ä»æ•°æ®åº“è·å–æ–‡æ¡£ä¿¡æ¯
            doc_info = self.db_manager.get_document_by_id(file_id)
            if not doc_info:
                return {"success": False, "error": "æ–‡æ¡£ä¸å­˜åœ¨"}
            
            # åˆ é™¤æ–‡ä»¶
            file_path = Path(doc_info["file_path"])
            if file_path.exists():
                file_path.unlink()
            
            # ä»å‘é‡æ•°æ®åº“ä¸­åˆ é™¤ç›¸å…³å‘é‡
            # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥å®ç°å‘é‡åˆ é™¤åŠŸèƒ½
            
            # ä»æ•°æ®åº“ä¸­åˆ é™¤è®°å½•
            if self.db_manager.delete_document(file_id):
                # ä»å†…å­˜ä¸­åˆ é™¤ï¼ˆå‘åå…¼å®¹ï¼‰
                if file_id in self.documents:
                    del self.documents[file_id]
                return {"success": True, "message": "æ–‡æ¡£åˆ é™¤æˆåŠŸ"}
            else:
                return {"success": False, "error": "æ•°æ®åº“åˆ é™¤å¤±è´¥"}
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£åˆ é™¤å¤±è´¥: {e}")
            return {"success": False, "error": f"åˆ é™¤å¤±è´¥: {str(e)}"}
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            stats = self.db_manager.get_stats()
            
            return {
                "success": True,
                "stats": {
                    "total_documents": stats['total_documents'],
                    "total_chunks": stats['total_chunks'],
                    "total_vectors": stats['total_vectors'],
                    "total_chats": stats['total_chats'],
                    "system_status": "è¿è¡Œæ­£å¸¸"
                }
            }
        except Exception as e:
            return {"success": False, "error": f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"}


# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size
CORS(app)  # å¯ç”¨è·¨åŸŸæ”¯æŒ

# åˆ›å»ºRAGæœåŠ¡å®ä¾‹
rag_service = RAGWebService()


@app.route('/')
def index():
    """ä¸»é¡µ"""
    return send_from_directory('static', 'index.html')


@app.route('/static/<path:filename>')
def static_files(filename):
    """é™æ€æ–‡ä»¶æœåŠ¡"""
    return send_from_directory('static', filename)


@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    try:
        # æ£€æŸ¥Ollamaå®¢æˆ·ç«¯çŠ¶æ€
        is_ollama_available = not hasattr(rag_service.ollama_client, 'is_mock')
        
        # è·å–Ollamaé…ç½®ä¿¡æ¯
        ollama_info = {
            "url": f"http://{rag_service.ollama_client.host}:{rag_service.ollama_client.port}",
            "chat_model": rag_service.ollama_client.chat_model,
            "embedding_model": rag_service.ollama_client.embedding_model,
            "available": is_ollama_available,
            "status": "connected" if is_ollama_available else "disconnected"
        }
        
        return jsonify({
            "status": "healthy" if is_ollama_available else "degraded",
            "timestamp": datetime.now().isoformat(),
            "service": "RAG Knowledge Base API",
            "ollama": ollama_info
        })
    except Exception as e:
        return jsonify({
            "status": "error",
            "timestamp": datetime.now().isoformat(),
            "service": "RAG Knowledge Base API",
            "error": str(e)
        }), 500


@app.route('/api/upload', methods=['POST'])
def upload_file():
    """æ–‡ä»¶ä¸Šä¼ æ¥å£"""
    try:
        if 'file' not in request.files:
            return jsonify({"success": False, "error": "æ²¡æœ‰æ–‡ä»¶"}), 400
        
        file = request.files['file']
        result = rag_service.upload_document(file)
        
        if result["success"]:
            return jsonify(result), 200
        else:
            return jsonify(result), 400
            
    except RequestEntityTooLarge:
        return jsonify({"success": False, "error": "æ–‡ä»¶å¤ªå¤§"}), 413
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route('/api/chat', methods=['POST'])
def chat():
    """å¯¹è¯æ¥å£"""
    try:
        # æ£€æŸ¥Ollamaå®¢æˆ·ç«¯çŠ¶æ€
        if hasattr(rag_service.ollama_client, 'is_mock'):
            return jsonify({
                "success": False, 
                "error": "OllamaæœåŠ¡æœªè¿æ¥ï¼Œè¯·æ£€æŸ¥Ollamaæ˜¯å¦æ­£å¸¸è¿è¡Œ"
            }), 503
        
        data = request.get_json()
        if not data or 'question' not in data:
            return jsonify({"success": False, "error": "ç¼ºå°‘é—®é¢˜å‚æ•°"}), 400
        
        question = data['question']
        top_k = data.get('top_k', 1)
        stream = data.get('stream', False)  # æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
        
        if stream:
            return stream_chat(question, top_k)
        else:
            result = rag_service.chat(question, top_k)
            
            if result["success"]:
                return jsonify(result), 200
            else:
                return jsonify(result), 400
            
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


def stream_chat(question, top_k=1):
    """æµå¼å¯¹è¯æ¥å£"""
    try:
        # æ£€æŸ¥Ollamaå®¢æˆ·ç«¯çŠ¶æ€
        if hasattr(rag_service.ollama_client, 'is_mock'):
            return jsonify({
                "success": False, 
                "error": "OllamaæœåŠ¡æœªè¿æ¥ï¼Œè¯·æ£€æŸ¥Ollamaæ˜¯å¦æ­£å¸¸è¿è¡Œ"
            }), 503
        
        # ç”Ÿæˆé—®é¢˜çš„å‘é‡
        question_vector = rag_service.ollama_client.get_embedding(question)
        if not question_vector:
            return jsonify({"success": False, "error": "é—®é¢˜å‘é‡åŒ–å¤±è´¥"}), 400
        
        # æœç´¢ç›¸å…³æ–‡æ¡£
        search_results = rag_service.vector_store.search_similar(
            query_vector=question_vector,
            top_k=top_k,
            score_threshold=0.3
        )
        
        if not search_results:
            return jsonify({
                "success": True,
                "answer": "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·å°è¯•ä¸Šä¼ ç›¸å…³æ–‡æ¡£æˆ–æ¢ä¸ªé—®é¢˜ã€‚",
                "sources": [],
                "question": question
            }), 200
        
        # æ„å»ºä¸Šä¸‹æ–‡ï¼Œè¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
        context_parts = []
        sources = []
        
        for result in search_results:
            payload = result.get('payload', {})
            text = payload.get('text', '')
            filename = payload.get('filename', 'æœªçŸ¥æ–‡ä»¶')
            chunk_index = payload.get('chunk_index', 0)
            score = result.get('score', 0)
            
            # è¿‡æ»¤ç›¸ä¼¼åº¦ä½äº0.5çš„ç»“æœ
            if score < 0.5:
                continue
                
            context_parts.append(text)
            sources.append({
                "filename": filename,
                "chunk_index": chunk_index,
                "score": round(score, 3),
                "text_preview": text[:200] + "..." if len(text) > 200 else text
            })
        
        # æ£€æŸ¥è¿‡æ»¤åæ˜¯å¦è¿˜æœ‰æœ‰æ•ˆç»“æœ
        if not context_parts:
            return jsonify({
                "success": True,
                "answer": "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·å°è¯•ä¸Šä¼ ç›¸å…³æ–‡æ¡£æˆ–æ¢ä¸ªé—®é¢˜ã€‚",
                "sources": [],
                "question": question
            }), 200
        
        context = "\n\n".join(context_parts)
        
        # æ„å»ºæç¤º
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ï¼š"""
        
        # ä½¿ç”¨æµå¼ç”Ÿæˆ
        def generate():
            # å…ˆè¿”å›å…ƒæ•°æ®
            metadata = json.dumps({
                "success": True,
                "sources": sources,
                "question": question,
                "metadata": True
            })
            yield f"data: {metadata}\n\n"
            
            # ç„¶åæµå¼è¿”å›ç”Ÿæˆçš„å†…å®¹
            full_answer = ""
            for text_chunk in rag_service.ollama_client.generate_stream_response(prompt):
                full_answer += text_chunk
                chunk_data = json.dumps({"chunk": text_chunk})
                yield f"data: {chunk_data}\n\n"
            
            # è®°å½•å¯¹è¯å†å²
            chat_record = {
                "id": str(uuid.uuid4()),
                "question": question,
                "answer": full_answer,
                "sources": sources,
                "timestamp": datetime.now().isoformat()
            }
            rag_service.chat_history.append(chat_record)
            
            # å‘é€å®Œæˆä¿¡å·
            done_data = json.dumps({"done": True})
            yield f"data: {done_data}\n\n"
        
        return app.response_class(
            generate(),
            mimetype='text/event-stream'
        )
        
    except Exception as e:
        print(f"âŒ æµå¼å¯¹è¯å¤„ç†å¤±è´¥: {e}")
        traceback.print_exc()
        return jsonify({"success": False, "error": f"æµå¼å¯¹è¯å¤„ç†å¤±è´¥: {str(e)}"}), 500


@app.route('/api/documents', methods=['GET'])
def get_documents():
    """
    è·å–æ–‡æ¡£åˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µï¼‰
    """
    try:
        # è·å–åˆ†é¡µå‚æ•°
        page = request.args.get('page', 1, type=int)
        page_size = request.args.get('page_size', 5, type=int)  # é»˜è®¤æ¯é¡µ5æ¡
        
        # å‚æ•°éªŒè¯
        if page < 1:
            page = 1
        if page_size < 1 or page_size > 50:  # é™åˆ¶æ¯é¡µæœ€å¤š50æ¡
            page_size = 5
        
        result = rag_service.get_documents(page, page_size)
        return jsonify(result)
    except Exception as e:
        print(f"âŒ è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": str(e),
            "documents": [],
            "pagination": {
                "current_page": 1,
                "page_size": 5,
                "total_items": 0,
                "total_pages": 0,
                "has_next": False,
                "has_prev": False
            }
        }), 500


@app.route('/api/documents/<file_id>', methods=['DELETE'])
def delete_document(file_id):
    """åˆ é™¤æ–‡æ¡£æ¥å£"""
    try:
        result = rag_service.delete_document(file_id)
        
        if result["success"]:
            return jsonify(result), 200
        else:
            return jsonify(result), 400
            
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route('/api/clear', methods=['POST'])
def clear_knowledge_base():
    """æ¸…ç†çŸ¥è¯†åº“æ¥å£"""
    try:
        result = rag_service.clear_knowledge_base()
        
        if result["success"]:
            return jsonify(result), 200
        else:
            return jsonify(result), 400
            
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route('/api/stats', methods=['GET'])
def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯æ¥å£"""
    try:
        result = rag_service.get_stats()
        
        if result["success"]:
            return jsonify(result), 200
        else:
            return jsonify(result), 500
            
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.errorhandler(404)
def not_found(error):
    """404é”™è¯¯å¤„ç†"""
    return jsonify({"success": False, "error": "æ¥å£ä¸å­˜åœ¨"}), 404


@app.errorhandler(500)
def internal_error(error):
    """500é”™è¯¯å¤„ç†"""
    return jsonify({"success": False, "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}), 500


if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨RAGçŸ¥è¯†åº“WebæœåŠ¡...")
    
    # æ£€æŸ¥ç»„ä»¶åˆå§‹åŒ–çŠ¶æ€
    if not rag_service.ollama_client:
        print("âš ï¸ éƒ¨åˆ†ç»„ä»¶åˆå§‹åŒ–å¤±è´¥ï¼Œä½†ç»§ç»­å¯åŠ¨æœåŠ¡")
    else:
        print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
    
    print("ğŸ“ APIæ¥å£:")
    print("   - GET  /api/health     - å¥åº·æ£€æŸ¥")
    print("   - POST /api/upload     - æ–‡ä»¶ä¸Šä¼ ")
    print("   - POST /api/chat       - å¯¹è¯æ¥å£")
    print("   - GET  /api/documents  - è·å–æ–‡æ¡£åˆ—è¡¨")
    print("   - DELETE /api/documents/<id> - åˆ é™¤æ–‡æ¡£")
    print("   - GET  /api/stats      - è·å–ç»Ÿè®¡ä¿¡æ¯")
    print("\nğŸŒ WebæœåŠ¡åœ°å€: http://localhost:5000")
    
    # æ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
    debug_mode = os.getenv('FLASK_ENV', 'development') == 'development'
    app.run(host='0.0.0.0', port=5000, debug=debug_mode)